version: '3.8'

networks:
  spark-overlay-net:
    driver: overlay
    attachable: true
    ipam:
      config:
        - subnet: 10.10.0.0/24

services:
  spark-master:
    image: minhtramnp/spark_image:v1
    networks:
      - spark-overlay-net
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          cpus: '1'
          memory: 4G
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    entrypoint: ["start-master.sh", "-p", "7077"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 5s
      timeout: 3s
      retries: 3
    env_file:
      - ./docker/.env.spark
    volumes:
      - type: bind
        source: ./spark_jobs
        target: /opt/spark/spark-jobs
      - type: bind
        source: ./data
        target: /opt/spark/data
      - type: volume
        source: spark-logs
        target: /opt/spark/spark-events
    ports:
      - target: 8080
        published: 8080
        protocol: tcp
        mode: host
      - target: 7077
        published: 7077
        protocol: tcp
        mode: host

  spark-worker:
    image: minhtramnp/spark_image:v1
    networks:
      - spark-overlay-net
    deploy:
      mode: replicated
      replicas: 2
      placement:
        constraints:
          - node.role == worker
      resources:
        limits:
          cpus: '1'
          memory: 2G
    entrypoint: ["start-worker.sh", "spark://spark-master:7077"]
    env_file:
      - ./docker/.env.spark
    volumes:
      - type: bind
        source: ./data
        target: /opt/spark/data
      - type: volume
        source: spark-logs
        target: /opt/spark/spark-events

  spark-history-server:
    image: minhtramnp/spark_image:v1
    networks:
      - spark-overlay-net
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
    entrypoint: ["start-history-server.sh"]
    env_file:
      - ./docker/.env.spark
    volumes:
      - type: volume
        source: spark-logs
        target: /opt/spark/spark-events
    ports:
      - target: 18080
        published: 18080
        protocol: tcp
        mode: host

  spark-jupyter:
    image: minhtramnp/spark_image:v1
    networks:
      - spark-overlay-net
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
    entrypoint: ["sh", "-c", "jupyter notebook --notebook-dir=/opt/spark --no-browser --ip=0.0.0.0 --port=8889 --allow-root"]
    env_file:
      - ./docker/.env.spark
    environment:
      PYSPARK_PYTHON: python3
    volumes:
      - type: bind
        source: ./notebooks
        target: /opt/spark/notebooks
      - type: volume
        source: spark-logs
        target: /opt/spark/spark-events
      - type: bind
        source: ./data
        target: /opt/spark/data
    ports:
      - target: 8889
        published: 8889
        protocol: tcp
        mode: host

  spark-submit:
    image: minhtramnp/spark_image:v1
    networks:
      - spark-overlay-net
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
    command: ["tail", "-f", "/dev/null"]
    env_file:
      - ./docker/.env.spark
    volumes:
      - type: bind
        source: ./spark_jobs
        target: /opt/spark/spark-jobs
      - type: bind
        source: ./data
        target: /opt/spark/data
      - type: volume
        source: spark-logs
        target: /opt/spark/spark-events

volumes:
  spark-logs:
    driver: local
