version: "3.8"

services:
  spark-master:
    user: root
    container_name: spark-master
    image: oanhnguyenlethanh/spark-image:v1.2
    command: ["start-master.sh", "-p", "7077"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 5s
      timeout: 3s
      retries: 3
    volumes:
      - ./spark-jobs:/opt/spark/spark_jobs:ro
      - ./data:/opt/spark/data:rw
      - spark-logs:/opt/spark/spark-events:rw
    env_file:
      - ./.env.spark
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - spark-net

  spark-history-server:
    user: root
    container_name: spark-history
    image: oanhnguyenlethanh/spark-image:v1.2
    command: ["start-history-server.sh"]
    depends_on:
      spark-master:
        condition: service_healthy
    env_file:
      - ./.env.spark
    volumes:
      - spark-logs:/opt/spark/spark-events:rw
    ports:
      - "18080:18080"
    networks:
      - spark-net

  spark-worker:
    user: root
    container_name: spark-worker
    image: oanhnguyenlethanh/spark-image:v1.2
    command: ["start-worker.sh", "spark://spark-master:7077"]
    depends_on:
      spark-master:
        condition: service_healthy
    env_file:
      - ./.env.spark
    volumes:
      - ./data:/opt/spark/data:rw
      - spark-logs:/opt/spark/spark-events:rw
    networks:
      - spark-net

  spark-jupyter:
    user: root
    # build: ./docker
    container_name: spark-jupyter
    image: oanhnguyenlethanh/spark-image:v1.2
    depends_on:
      - spark-master
    ports:
      - "8889:8889"
    volumes:
      - ./notebooks:/opt/spark/notebooks
      - spark-logs:/opt/spark/spark-events:rw
    environment:
      PYSPARK_PYTHON: python3
    command: "jupyter notebook --no-browser --ip=0.0.0.0 --port=8889 --allow-root"
    env_file:
      - ./.env.spark
    networks:
      - spark-net

  spark-submit:
    user: root
    image: oanhnguyenlethanh/spark-image:v1.2
    container_name: spark-submit
    depends_on:
      - spark-master
    volumes:
      - ./spark-jobs:/opt/spark/spark_jobs
      - spark-logs:/opt/spark/spark-events:rw
    command: ["spark-submit", "/opt/spark/spark-jobs/cluster_analysis.py"]
    env_file:
      - ./.env.spark
    networks:
      - spark-net

networks:
  spark-net:
    driver: bridge

volumes:
  spark-logs:
